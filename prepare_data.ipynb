{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from utilities.visualizers import view_label_freq\n",
    "from utilities.loaders import load_labels, save_model, concur_load_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load audio signals and respective labels for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 16000 samples per second originally but\n",
    "# if we let librosa interpolate our signals it would be 256hz\n",
    "# which is frequency typical to that of a human voice\n",
    "hertz = 8000\n",
    "\n",
    "# how many seconds we want our window to be\n",
    "# e.g. if we want our signal segment to be 1 second\n",
    "# then this would mean 16000 (or 22050) samples that we need to aggregate\n",
    "# quarter of a second\n",
    "window_time = 0.25\n",
    "\n",
    "# how many seconds we want our signal segments to overlap\n",
    "# one eighth of a second (1/8)\n",
    "hop_time = 0.125\n",
    "\n",
    "# note that the shorter the window time and hop time the more there will be data points in our final dataset\n",
    "# which can be computationally intensive for our machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./data/\"\n",
    "folders = list(filter(lambda file: not file.endswith(\".tgz\") and (not \"_EXTRACTED_FEATURES\" in file), os.listdir(DIR)))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load_labels(DIR, folders)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(labels, columns=[\"subject_name\", \"string\", \"label\"])\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"string\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[labels_df[\"label\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once all .tar file contents are extracted we proceed to trimming any insignificant parts of the audio signal and have it be of the same length as the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis as kurt, skew, mode, entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utilities.loaders import load_audio\n",
    "from utilities.preprocessors import encode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = load_audio(DIR, folders, hertz=hertz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of newly combined dataset for each subject will have a longer vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df = pd.DataFrame(signals, columns=[\"subject_name\", \"raw_signals\"])\n",
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = signals_df.merge(labels_df, how=\"left\", on=[\"subject_name\"])\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset_df[\"label\"].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dataset_df[\"label\"].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_label_freq(dataset_df[\"label\"].value_counts(), img_title=\"male to female ratio\", save_img=True, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"label\"], dataset_df_le = encode_features(dataset_df[\"label\"])\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see 1 is male and 0 is female and 2 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_le.inverse_transform([0, 0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We save this encoder for later when we run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dataset_df_le, './saved/misc/audio_dataset_le.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df, test_dataset_df = train_test_split(dataset_df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "librosa.display.waveshow(dataset_df.loc[0, \"raw_signals\"], alpha=0.5, color=\"#8442f5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_dataset_df.itertuples(index=False, name=None))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = list(test_dataset_df.itertuples(index=False, name=None))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All we havee to do now is to extract the features of each combined vector for each subject\n",
    "### 521216 is the length of the 16000hz test audio signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.feature_extractors import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_final = extract_features(train_dataset, hertz=hertz, window_time=window_time, hop_time=hop_time)\n",
    "train_dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_final[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_final[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_final[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This merges the list of features returned by `extract_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_merged = pd.concat(train_dataset_final[0], axis=0, ignore_index=True)\n",
    "# train_features_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This merges the list of labels returned by `extract_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels_merged = pd.concat(train_dataset_final[1], axis=0, ignore_index=True)\n",
    "# train_labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels_merged.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving extracted features to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('./data/_EXTRACTED_FEATURES/', exist_ok=True)\n",
    "# train_features_merged.to_csv('./data/_EXTRACTED_FEATURES/train_features_merged.csv')\n",
    "# train_labels_merged.to_csv('./data/_EXTRACTED_FEATURES/train_labels_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do feature extraction on test set also and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_final = extract_features(test_dataset, hertz=hertz, window_time=window_time, hop_time=hop_time)\n",
    "test_dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features_merged = pd.concat(test_dataset_final[0], axis=0, ignore_index=True)\n",
    "# test_features_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels_merged = pd.concat(test_dataset_final[1], axis=0, ignore_index=True)\n",
    "# test_labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features_merged.to_csv('./data/_EXTRACTED_FEATURES/test_features_merged.csv')\n",
    "# test_labels_merged.to_csv('./data/_EXTRACTED_FEATURES/test_labels_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We visualize the calculated root mean squared energy of the audio signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "librosa.display.waveshow(dataset_df.loc[0, \"raw_signals\"], alpha=0.5, color=\"#2ddae3\")\n",
    "\n",
    "time = train_dataset_final[2][0]\n",
    "plt.scatter(time, train_dataset_final[0][0][\"rms\"], color=\"#6cf542\", marker='.', alpha=1)\n",
    "plt.plot(time, train_dataset_final[0][0][\"rms\"], color=\"#e02f8e\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures & images/root mean squared energy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero crossing rate feature of audio signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "time = train_dataset_final[2][0]\n",
    "plt.scatter(time, train_dataset_final[0][0][\"zcr\"], color=\"#6cf542\", marker='.', alpha=1)\n",
    "plt.plot(time, train_dataset_final[0][0][\"zcr\"], color=\"#e02f8e\", alpha=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures & images/zero crossing rate feature.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "time = train_dataset_final[2][0]\n",
    "plt.scatter(time, train_dataset_final[0][0][\"mean_mel\"], color=\"#6cf542\", marker='.', alpha=1)\n",
    "plt.plot(time, train_dataset_final[0][0][\"mean_mel\"], color=\"#e02f8e\", alpha=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures & images/mel frequency mean feature.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance of mel frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "time = train_dataset_final[2][0]\n",
    "plt.scatter(time, train_dataset_final[0][0][\"variance_mel\"], color=\"#6cf542\", marker='.', alpha=1)\n",
    "plt.plot(time, train_dataset_final[0][0][\"variance_mel\"], color=\"#e02f8e\", alpha=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures & images/mel frequency variance feature.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "time = train_dataset_final[2][0]\n",
    "plt.scatter(time, train_dataset_final[0][0][\"spect_cent\"], color=\"#6cf542\", marker='.', alpha=1)\n",
    "plt.plot(time, train_dataset_final[0][0][\"spect_cent\"], color=\"#e02f8e\", alpha=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./figures & images/spectral centroid feature.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For deep learning however and the use of LSTMs and CNNs and of its ilk we can use raw audio signals themselves to extract deep features from. As we know these models are able to extract higher order features automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_dataset_final = concur_load_data(train_dataset, hertz=hertz, window_time=window_time, hop_time=hop_time, config=\"deep\")\n",
    "# dl_train_dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_signals_merged = np.concatenate(dl_train_dataset_final[0], axis=0)\n",
    "# dl_train_signals_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_labels_merged = np.concatenate(dl_train_dataset_final[1], axis=0)\n",
    "# dl_train_labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # again in lstms we have the concept of timesteps\n",
    "# # but in the case of signal processing especially biosignal\n",
    "# # and audio processing the number of features can sometimes be\n",
    "# # just 1 dimension and the timesteps could be the window size itself\n",
    "# m, tx, nf  = dl_train_signals_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl_train_labels_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m, tx, nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datung-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
